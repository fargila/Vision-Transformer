{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO76Rr7bNy8itAgQ4J7pJAq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fargila/Vision-Transformer/blob/main/Vision_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Building a vision transformer with CIFAR-10 dataset\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "V1v4FM7Mts9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "lGwa7QHiu5D2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sga4P5NJtfdc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from ast import Param\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Device-Agnostic Code"
      ],
      "metadata": {
        "id": "UXgGlJzxvDVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "-EO0e2ZsvMbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting the seed\n"
      ],
      "metadata": {
        "id": "5eHmKQCXvwNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "CzU_37obv0mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting the hyperparameters"
      ],
      "metadata": {
        "id": "UpJyC7ScwKqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 3e-4\n",
        "PATCH_SIZE = 4\n",
        "NUM_CLASSES = 10\n",
        "NUM_HEADS = 8\n",
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 3\n",
        "EMBED_DIM = 256\n",
        "DEPTH = 6\n",
        "MLP_DIM = 512\n",
        "DROP_RATE = 0.1"
      ],
      "metadata": {
        "id": "OlS4_2kswSwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define image transformations\n"
      ],
      "metadata": {
        "id": "Wk3Xu10sxDMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.5), (0.5))\n",
        "# ])\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5), (0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "L9HoN74-xIy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting a dataset"
      ],
      "metadata": {
        "id": "tVdk08cLxgLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trains_dataset = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root=\"data\", train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "rlZpkGI6xjFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting the datasets into dataloaders"
      ],
      "metadata": {
        "id": "xVfuwPQr2Jo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(trains_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"Data loader: {train_loader, test_loader}\")\n",
        "print(f\"Length of train loader: {len(train_loader)} batches of {BATCH_SIZE}...\")\n",
        "print(f\"Length of test loader: {len(test_loader)} batches of {BATCH_SIZE}...\")"
      ],
      "metadata": {
        "id": "bieLGjJY2SHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the **Vision Transformer**"
      ],
      "metadata": {
        "id": "MG3lubG-5O6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  def __init__(self, img_size, patch_size, in_channels, embed_dim):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "    self.proj = nn.Conv2d(in_channels,  embed_dim,  kernel_size=patch_size, stride=patch_size)\n",
        "    num_patches = (img_size // patch_size) ** 2\n",
        "    self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "    self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    B = x.size(0)\n",
        "    x = self.proj(x)\n",
        "    x = x.flatten(2).transpose(1, 2)\n",
        "    cls_token = self.cls_token.expand(B, -1, -1)\n",
        "    x = torch.cat((cls_token, x), dim=1)\n",
        "    x += self.pos_embed\n",
        "    return x"
      ],
      "metadata": {
        "id": "zDLzBsSB5YSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, in_features, hidden_features, drop_rate):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features, out_features=hidden_features)\n",
        "    self.gelu = nn.GELU() # Changed from F.gelu\n",
        "    self.fc2 = nn.Linear(in_features=hidden_features, out_features=in_features)\n",
        "    self.drop = nn.Dropout(drop_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Changed F.gelu(self.fc1(x)) to self.gelu(self.fc1(x))\n",
        "    x = self.drop(self.gelu(self.fc1(x)))\n",
        "    x = self.drop(self.fc2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "klBXYM5P8l87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, embed_dim, mlp_dim, num_heads, drop_rate):\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.LayerNorm(embed_dim)\n",
        "    self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=drop_rate, batch_first=True)\n",
        "    self.norm2 = nn.LayerNorm(embed_dim)\n",
        "    self.mlp = MLP(embed_dim, mlp_dim, drop_rate)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Added .clone() after the attention layer output\n",
        "    x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0].clone()\n",
        "    x = x + self.mlp(self.norm2(x))\n",
        "    return x"
      ],
      "metadata": {
        "id": "zcFq2rG97JNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self, img_size, patch_size, in_channels, num_classes,\n",
        "                      embed_dim, depth, num_heads, mlp_dim, drop_rate):\n",
        "    super().__init__()\n",
        "    self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "    self.encoder = nn.Sequential(*[TransformerEncoderLayer(embed_dim,\n",
        "                                                      mlp_dim, num_heads, drop_rate)\n",
        "      for _ in range(depth)\n",
        "    ])\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "    self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.patch_embed(x)\n",
        "    x = self.encoder(x)\n",
        "    x = self.norm(x)\n",
        "    cls_token = x[:, 0]\n",
        "    return self.fc(cls_token)"
      ],
      "metadata": {
        "id": "7Z8S51sL8UrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VisionTransformer(IMAGE_SIZE, PATCH_SIZE, CHANNELS, NUM_CLASSES, EMBED_DIM,\n",
        "                                             DEPTH, NUM_HEADS, MLP_DIM, DROP_RATE).to(device)"
      ],
      "metadata": {
        "id": "6gB4fRFg-sJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining a loss function and an optimizer"
      ],
      "metadata": {
        "id": "DAn1QKN4CJiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "oG6wLgH0CSiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining a Training Loop function"
      ],
      "metadata": {
        "id": "lXCpcUESCukJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, criterion):\n",
        "  model.train()\n",
        "  total_loss, correct = 0, 0\n",
        "\n",
        "  for x, y in loader:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    out = model(x)\n",
        "    loss = criterion(out, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item() * x.size(0)\n",
        "    correct += (out.argmax(1) == y).sum().item()\n",
        "  return total_loss / len(loader.dataset), correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "dQL3gijaCywP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  with torch.inference_mode():\n",
        "    for x, y in loader:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      out = model(x)\n",
        "      correct += (out.argmax(dim=1) == y).sum().item()\n",
        "  return correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "DYsfsOf0FgM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "6hb5hw0tHhEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracies, test_accuracies = [], []\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "  train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "  test_acc = evaluate(model, test_loader)\n",
        "  train_accuracies.append(train_acc)\n",
        "  test_accuracies.append(test_acc)\n",
        "  print(f\"Epoch: {epoch + 1}/{EPOCHS}, Train Loss: {train_loss:.4f},\"\n",
        "           f\"Train Acc: {train_acc:.4f}%, Test Acc: {test_acc:.4f}%\")"
      ],
      "metadata": {
        "id": "En7-0QeKGScd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_accuracies, label=\"Train accuracy\")\n",
        "plt.plot(test_accuracies, label=\"Test accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and test accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jKc2Oc6UKRnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, dataset, classes, grid_size=3):\n",
        "  model.eval()\n",
        "  fig, plt.axes = plt.subplots(grid_size, grid_size, figsize=(10, 10))\n",
        "\n",
        "  for i in range(grid_size):\n",
        "    for j in range(grid_size):\n",
        "      idx = random.randint(0, len(dataset) - 1)\n",
        "      img, true_label = dataset[idx]\n",
        "      input_tensor = img.unsqueeze(dim=0).to(device)\n",
        "      with torch.inference_mode():\n",
        "        output = model(input_tensor)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "      img = img / 2 + 0.5\n",
        "      npimg = img.cpu().numpy()\n",
        "      plt.axes[i, j].imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "      truth = classes[true_label] == classes[predicted.item()]\n",
        "\n",
        "      if truth: color = \"g\"\n",
        "      else: color = \"r\"\n",
        "      plt.axes[i, j].set_title(f\"Truth: {classes[true_label]}\\n, Predicted: {classes[predicted.item()]}\",\n",
        "      fontsize=10, c=color)\n",
        "      plt.axes[i, j].axis(\"off\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "V4TRnEQwLL44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model, test_dataset, classes=trains_dataset.classes, grid_size=3)"
      ],
      "metadata": {
        "id": "1i2EqTn7OrSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For more in-depth information, go to **https://docs.pytorch.org/vision/0.9/transforms.html**"
      ],
      "metadata": {
        "id": "idbxpJqRPkC9"
      }
    }
  ]
}